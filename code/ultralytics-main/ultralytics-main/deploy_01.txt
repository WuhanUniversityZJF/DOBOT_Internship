#!/usr/bin/env python
import cv2
import numpy as np
from hobot_dnn import pyeasy_dnn as dnn
import logging
from time import time

# -------------------------- 日志配置 --------------------------
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S')
logger = logging.getLogger("YOLOv11-Pose-Detector")

# -------------------------- 关键点配置（根据模型训练时的定义修改） --------------------------
KEYPOINT_NAMES = [
    "nose", "left_eye", "right_eye", "left_ear", "right_ear",
    "left_shoulder", "right_shoulder", "left_elbow", "right_elbow",
    "left_wrist", "right_wrist", "left_hip", "right_hip",
    "left_knee", "right_knee", "left_ankle", "right_ankle",
    "head_top", "neck", "spine", "waist", "left_hand", "right_hand", "tail"
]  # 24个关键点名称（与模型匹配）

KEYPOINT_COLORS = [
    (255,0,0), (0,255,0), (0,0,255), (255,255,0), (255,0,255), (0,255,255),
    (128,0,0), (0,128,0), (0,0,128), (128,128,0), (128,0,128), (0,128,128),
    (255,128,0), (255,0,128), (128,255,0), (0,255,128), (128,0,255), (0,128,255),
    (255,255,128), (255,128,255), (128,255,255), (192,192,192), (255,192,128), (128,192,255)
]  # 24个关键点颜色

SKELETON = [
    (5,6), (5,7), (7,9), (6,8), (8,10),  # 肩-肘-腕
    (5,11), (6,12), (11,12), (11,13), (13,15),  # 肩-髋-膝-踝
    (12,14), (14,16), (0,1), (0,2), (1,3), (2,4)  # 面部+身体连接
]  # 骨骼连接（根据模型训练时的骨架定义修改）


class YOLOv11PoseDetector:
    def __init__(self, model_path, conf_thres=0.15, iou_thres=0.45):
        """初始化模型和参数"""
        self.conf_thres = conf_thres  # 置信度阈值（降低至0.15确保捕捉目标）
        self.iou_thres = iou_thres    # NMS IoU阈值
        self.num_keypoints = len(KEYPOINT_NAMES)  # 关键点数量（24）

        # 加载模型
        self.model = dnn.load(model_path)[0]
        logger.info(f"✅ 成功加载模型: {model_path}")

        # 获取模型输入尺寸 (N, C, H, W)
        self.input_tensor = self.model.inputs[0]
        self.input_shape = self.input_tensor.properties.shape
        self.target_h, self.target_w = self.input_shape[2], self.input_shape[3]  # 模型输入H和W
        logger.info(f"模型输入尺寸: (H={self.target_h}, W={self.target_w})")

        # 预计算YUV通道长度（NV12转换用）
        self.y_size = self.target_h * self.target_w
        self.uv_size = (self.target_h // 2) * (self.target_w // 2)
        self.total_size = self.y_size + 2 * self.uv_size

    def preprocess(self, bgr_img):
        """预处理：BGR→NV12（模型输入格式）"""
        # 1. 缩放图像到模型输入尺寸
        img_resized = cv2.resize(bgr_img, (self.target_w, self.target_h), interpolation=cv2.INTER_LINEAR)
        if img_resized.shape[:2] != (self.target_h, self.target_w):
            raise ValueError(f"图像缩放失败: 目标{(self.target_h,self.target_w)}，实际{img_resized.shape[:2]}")

        # 2. BGR→YUV420P（I420）→ 展平为1D数组
        yuv420p = cv2.cvtColor(img_resized, cv2.COLOR_BGR2YUV_I420)
        yuv420p = yuv420p.flatten()
        if yuv420p.size != self.total_size:
            raise ValueError(f"YUV尺寸错误: 理论{self.total_size}，实际{yuv420p.size}")

        # 3. 转换为NV12格式
        nv12 = np.empty(self.total_size, dtype=np.uint8)
        nv12[:self.y_size] = yuv420p[:self.y_size]  # Y通道
        nv12[self.y_size::2] = yuv420p[self.y_size : self.y_size+self.uv_size]  # U通道（偶数索引）
        nv12[self.y_size+1::2] = yuv420p[self.y_size+self.uv_size:]  # V通道（奇数索引）
        return nv12

    def postprocess(self, outputs, original_size):
        """后处理：解析模型输出→关键点坐标→终端打印"""
        original_h, original_w = original_size

        # -------------------------- 解析检测框输出（Output 0: (1,1,77,8400)）--------------------------
        det_output = outputs[0]  # 原始形状 (1,1,77,8400)
        det_output = det_output.transpose(0, 3, 1, 2)  # 转换为 (1,8400,1,77)
        dets = det_output.reshape(-1, 77)  # 展平为 (8400,77) → 8400个候选目标

        # -------------------------- 解析关键点输出（Output 4: (1,72,8400,1)）--------------------------
        kpt_output = outputs[4]  # 原始形状 (1,72,8400,1)
        kpt_output = kpt_output.transpose(0, 2, 1, 3)  # 转换为 (1,8400,72,1)
        kpts = kpt_output.reshape(-1, 72)  # 展平为 (8400,72) → 与dets一一对应（每个目标72个值=24关键点×3）

        # -------------------------- 筛选有效目标 --------------------------
        if len(dets) == 0 or len(kpts) == 0:
            logger.warning("未检测到任何候选目标")
            return []
        if len(dets) != len(kpts):
            logger.error(f"目标数量不匹配: dets={len(dets)}, kpts={len(kpts)}")
            return []

        # 筛选置信度>阈值的目标（dets第5列是置信度）
        valid_mask = dets[:, 4] > self.conf_thres
        valid_dets = dets[valid_mask]
        valid_kpts = kpts[valid_mask]

        if len(valid_kpts) == 0:
            logger.warning(f"未检测到置信度> {self.conf_thres} 的目标，请降低阈值")
            return []

        # -------------------------- 终端打印关键点坐标 --------------------------
        logger.info("\n" + "="*80 + "\n【关键点坐标】(x: 水平坐标, y: 垂直坐标, confidence: 置信度)")
        keypoints_list = []
        for obj_idx, (det, kpt_raw) in enumerate(zip(valid_dets, valid_kpts)):
            # 解析当前目标的关键点（24个关键点，每个关键点(x,y,confidence)）
            kpt = kpt_raw.reshape(self.num_keypoints, 3)  # 形状 (24, 3)

            # 坐标转换：归一化坐标 → 原图坐标（x = x_norm * 原图宽, y = y_norm * 原图高）
            kpt[:, 0] *= original_w  # x坐标
            kpt[:, 1] *= original_h  # y坐标
            keypoints_list.append(kpt)

            # 打印当前目标的关键点（按名称+坐标+置信度）
            logger.info(f"\n目标 {obj_idx+1}（置信度: {det[4]:.3f}）:")
            for kp_idx in range(self.num_keypoints):
                x, y, conf = kpt[kp_idx]
                if conf > 0.1:  # 只打印置信度>0.1的关键点
                    logger.info(f"  {KEYPOINT_NAMES[kp_idx]:<15} x: {int(x):4d}, y: {int(y):4d}, confidence: {conf:.3f}")

        return keypoints_list

    def draw_pose(self, bgr_img, keypoints_list):
        """可视化：在原图上绘制关键点和骨骼连接"""
        img_copy = bgr_img.copy()
        for kpts in keypoints_list:
            # 1. 绘制关键点
            for kp_idx in range(self.num_keypoints):
                x, y, conf = kpts[kp_idx]
                if conf < 0.1:  # 跳过置信度低的关键点
                    continue
                color = KEYPOINT_COLORS[kp_idx]
                # 绘制关键点（中心圆+边框）
                cv2.circle(img_copy, (int(x), int(y)), 6, color, -1)  # 内部填充
                cv2.circle(img_copy, (int(x), int(y)), 8, (255,255,255), 2)  # 白色边框
                # 绘制关键点索引（方便调试）
                cv2.putText(img_copy, f"{kp_idx}", (int(x)+10, int(y)-10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 2)

            # 2. 绘制骨骼连接
            for (kp1, kp2) in SKELETON:
                x1, y1, c1 = kpts[kp1]
                x2, y2, c2 = kpts[kp2]
                if c1 < 0.1 or c2 < 0.1:  # 跳过置信度低的连接
                    continue
                color = KEYPOINT_COLORS[kp1]
                cv2.line(img_copy, (int(x1), int(y1)), (int(x2), int(y2)), color, 3)  # 骨骼线

        return img_copy

    def infer(self, img_path, save_path):
        """完整推理流程：读取图像→预处理→推理→后处理→可视化→保存"""
        # 1. 读取原图
        bgr_img = cv2.imread(img_path)
        if bgr_img is None:
            raise FileNotFoundError(f"无法读取图像文件: {img_path}")
        original_h, original_w = bgr_img.shape[:2]
        logger.info(f"原图尺寸: (高度={original_h}, 宽度={original_w})")

        # 2. 预处理（BGR→NV12）
        begin_time = time()
        input_nv12 = self.preprocess(bgr_img)
        logger.info(f"预处理耗时: {1000*(time()-begin_time):.2f} ms")

        # 3. 模型推理
        begin_time = time()
        outputs = self.model.forward({self.input_tensor.name: input_nv12})
        # 反量化：将int8输出转换为float32（使用模型提供的scale）
        outputs_np = [out.buffer.astype(np.float32) * out.properties.scale_data for out in outputs]
        logger.info(f"推理耗时: {1000*(time()-begin_time):.2f} ms")

        # 4. 后处理（解析关键点+终端打印）
        begin_time = time()
        keypoints_list = self.postprocess(outputs_np, (original_h, original_w))
        logger.info(f"后处理耗时: {1000*(time()-begin_time):.2f} ms")

        # 5. 可视化并保存结果
        result_img = self.draw_pose(bgr_img, keypoints_list)
        cv2.imwrite(save_path, result_img)
        logger.info(f"\n处理完成！结果图像已保存至: {save_path}")
        return result_img


# -------------------------- 主函数：运行测试 --------------------------
if __name__ == "__main__":
    try:
        # 配置路径（根据实际情况修改）
        MODEL_PATH = "/home/sunrise/Desktop/dog_pose.bin"  # 模型路径
        IMG_PATH = "/home/sunrise/Desktop/1.jpeg"          # 输入图像路径
        SAVE_PATH = "/home/sunrise/Desktop/pose_result.jpg"  # 结果保存路径

        # 初始化检测器并执行推理
        detector = YOLOv11PoseDetector(
            model_path=MODEL_PATH,
            conf_thres=0.15,  # 置信度阈值（可根据效果调整）
            iou_thres=0.45    # NMS阈值
        )
        detector.infer(img_path=IMG_PATH, save_path=SAVE_PATH)

    except Exception as e:
        logger.error(f"运行失败: {str(e)}", exc_info=True)